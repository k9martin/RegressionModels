---
title: "Regression Models Quiz 3"
author: "Francisco Martín"
date: "October, 2018"
output: html_document
---

## 1.-

Consider the mtcars data set. Fit a model with mpg as the outcome that includes number of cylinders as a factor variable and weight as confounder. Give the adjusted estimate for the expected change in mpg comparing 8 cylinders to 4.


- 33.991

- -3.206

- -4.256

- -6.071                (Correct)

Solution ->

``` {r prove_1}
suppressPackageStartupMessages(library(datasets))
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
round(summary(fit)$coefficient,3)
```

## 2.-

Consider the mtcars data set. Fit a model with mpg as the outcome that includes number of cylinders as a factor variable and weight as a possible confounding variable. Compare the effect of 8 versus 4 cylinders on mpg for the adjusted and unadjusted by weight models. Here, adjusted means including the weight variable as a term in the regression model and unadjusted means the model without weight included. What can be said about the effect comparing 8 and 4 cylinders after looking at models with and without weight included?.

- Within a given weight, 8 cylinder vehicles have an expected 12 mpg drop in fuel efficiency.

- Holding weight constant, cylinder appears to have more of an impact on mpg than if weight is disregarded.

- Including or excluding weight does not appear to change anything regarding the estimated impact of number of cylinders on mpg.

- Holding weight constant, cylinder appears to have less of an impact on mpg than if weight is disregarded.     (Correct)

Solution ->

``` {r prove_2}
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
round(summary(fit2)$coefficient,3)[3]
```


## 3.-

Consider the mtcars data set. Fit a model with mpg as the outcome that considers number of cylinders as a factor variable and weight as confounder. Now fit a second model with mpg as the outcome model that considers the interaction between number of cylinders (as a factor variable) and weight. Give the P-value for the likelihood ratio test comparing the two models and suggest a model using 0.05 as a type I error rate significance benchmark.


- The P-value is larger than 0.05. So, according to our criterion, we would fail to reject, which suggests that the interaction terms may not be necessary.

- The P-value is small (less than 0.05). So, according to our criterion, we reject, which suggests that the interaction term is not necessary.

- The P-value is small (less than 0.05). Thus it is surely true that there is an interaction term in the true model.

- The P-value is small (less than 0.05). So, according to our criterion, we reject, which suggests that the interaction term is necessary

- The P-value is larger than 0.05. So, according to our criterion, we would fail to reject, which suggests that the interaction terms is necessary.     (Correct)

- The P-value is small (less than 0.05). Thus it is surely true that there is no interaction term in the true model.

Solution ->

``` {r prove_3}
fit2 <- lm(mpg~factor(cyl)*wt,mtcars)
anova(fit,fit2)
```

## 4.-

Consider the mtcars data set. Fit a model with mpg as the outcome that includes number of cylinders as a factor variable and weight inlcuded in the model as

`lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)`

How is the wt coefficient interpretted?

- The estimated expected change in MPG per one ton increase in weight.

- The estimated expected change in MPG per half ton increase in weight for the average number of cylinders.

- The estimated expected change in MPG per half ton increase in weight.

- The estimated expected change in MPG per half ton increase in weight for for a specific number of cylinders (4, 6, 8).

- The estimated expected change in MPG per one ton increase in weight for a specific number of cylinders (4, 6, 8).             (Correct)


Solution -> 

``` {r prove_4}
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(fit4)$coefficient
```

## 5.-

Consider the following data set

``` {r eval=FALSE}
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
``` 

Give the hat diagonal for the most influential point


- 0.9946        (Correct)

- 0.2025

- 0.2287

- 0.2804


Solution ->

``` {r prove_5}
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
round(hatvalues(fit5),4)

```

## 6.-

Consider the following data set

``` {r eval=FALSE}
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
```

Give the slope dfbeta for the point with the highest hat value.


- -134  (Correct)

- -0.378

- -.00134

- 0.673

Solution ->

``` {r prove_6}
round(dfbetas(fit5)[, 2])
```

## 7.-

Consider a regression relationship between Y and X with and without adjustment for a third variable Z. Which of the following is true about comparing the regression coefficient between Y and X with and without adjustment for Z.

- The coefficient can't change sign after adjustment, except for slight numerical pathological cases.

- It is possible for the coefficient to reverse sign after adjustment. For example, it can be strongly significant and positive before adjustment and strongly significant and negative after adjustment.             (Correct)

- Adjusting for another variable can only attenuate the coefficient toward zero. It can't materially change sign.

- For the the coefficient to change sign, there must be a significant interaction term.


Solution -> It autoexplains.